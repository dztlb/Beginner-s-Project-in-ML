# åŸºäºæœºå™¨å­¦ä¹ çš„MgCo2O4ç”µæææ–™æ€§èƒ½é¢„æµ‹ä¸å½¢è²Œæ§åˆ¶é¡¹ç›®è¯¦ç»†è¯´æ˜æ–‡æ¡£
## ä¸€ã€é¡¹ç›®èƒŒæ™¯æè¿°

### 1.1 é¡¹ç›®èƒŒæ™¯

#### 1.1.1 è¶…çº§ç”µå®¹å™¨æŠ€æœ¯èƒŒæ™¯
è¶…çº§ç”µå®¹å™¨å‡­å€Ÿé«˜åŠŸç‡å¯†åº¦ã€å¿«é€Ÿå……æ”¾ç”µé€Ÿç‡å’Œé•¿å¾ªç¯å¯¿å‘½çš„ç‰¹ç‚¹ï¼Œå·²æˆä¸ºæ–°å‹å‚¨èƒ½é¢†åŸŸçš„å…³é”®è§’è‰²ã€‚åœ¨è¶…çº§ç”µå®¹å™¨èƒ½é‡å­˜å‚¨çš„ç”µåŒ–å­¦è¿‡ç¨‹ä¸­ï¼Œç”µæææ–™å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å®ƒä¸ä»…å†³å®šäº†æœ€ç»ˆå™¨ä»¶çš„æˆæœ¬ï¼Œè¿˜å¯¹ç”µå®¹æ€§èƒ½æœ‰ç€å†³å®šæ€§å½±å“ã€‚

**æŠ€æœ¯ä¼˜åŠ¿**ï¼š
- **é«˜åŠŸç‡å¯†åº¦**ï¼šå¯è¾¾10-100 kW/kg
- **å¿«é€Ÿå……æ”¾ç”µ**ï¼šç§’çº§å……æ”¾ç”µèƒ½åŠ›
- **é•¿å¾ªç¯å¯¿å‘½**ï¼š>100,000æ¬¡å¾ªç¯
- **ç¯ä¿å‹å¥½**ï¼šæ— æœ‰å®³ç‰©è´¨æ’æ”¾

#### 1.1.2 MgCo2O4ææ–™ç‰¹æ€§
MgCo2O4ä½œä¸ºä¸€ç§æå…·æ½œåŠ›çš„ç”µæææ–™ï¼Œå…·æœ‰ä»¥ä¸‹çªå‡ºä¼˜åŠ¿ï¼š

**ç‰©ç†åŒ–å­¦ç‰¹æ€§**ï¼š
- **ç†è®ºæ¯”ç”µå®¹**ï¼šé«˜è¾¾3122F/g
- **æ™¶ä½“ç»“æ„**ï¼šå°–æ™¶çŸ³ç»“æ„ï¼Œç¨³å®šæ€§å¥½
- **ç”µå¯¼ç‡**ï¼šè‰¯å¥½çš„ç”µå­å’Œç¦»å­ä¼ å¯¼èƒ½åŠ›
- **å½¢è²Œå¤šæ ·æ€§**ï¼šå¯å½¢æˆ1Dã€2Dã€3Då¤šç§å½¢è²Œ

**åº”ç”¨å‰æ™¯**ï¼š
- ç§»åŠ¨è®¾å¤‡å‚¨èƒ½
- ç”µåŠ¨æ±½è½¦åŠ¨åŠ›ç³»ç»Ÿ
- å¯å†ç”Ÿèƒ½æºå­˜å‚¨
- å·¥ä¸šç”µåŠ›ç³»ç»Ÿ

#### 1.1.3 æœºå™¨å­¦ä¹ åº”ç”¨ä»·å€¼
é‰´äºæœºå™¨å­¦ä¹ æŠ€æœ¯çš„è¿…çŒ›å‘å±•ï¼Œä»¥åŠé’ˆå¯¹MgCo2O4ææ–™å¼€å±•æ·±å…¥ç ”ç©¶çš„è¿«åˆ‡éœ€æ±‚ï¼Œæœ¬é¡¹ç›®è¿ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯å¯¹MgCo2O4çš„ææ–™æ€§è´¨å±•å¼€é¢„æµ‹å’Œåˆ†æã€‚

**MLåº”ç”¨ä¼˜åŠ¿**ï¼š
- **åŠ é€Ÿææ–™å‘ç°**ï¼šä»ä¼ ç»Ÿè¯•é”™åˆ°æ™ºèƒ½é¢„æµ‹
- **é™ä½æˆæœ¬**ï¼šå‡å°‘å®éªŒæ¬¡æ•°å’Œææ–™æµªè´¹
- **æé«˜ç²¾åº¦**ï¼šå¤šå› ç´ ç»¼åˆåˆ†æ
- **æœºç†æ­ç¤º**ï¼šç‰¹å¾é‡è¦æ€§åˆ†æ

### 1.2 é¡¹ç›®ä»»åŠ¡è¦æ±‚

#### 1.2.1 æ ¸å¿ƒä»»åŠ¡
æœ¬é¡¹ç›®ä¸»è¦åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼š

1. **æ¯”ç”µå®¹é¢„æµ‹ä»»åŠ¡**ï¼šæ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹MgCo2O4ç”µæææ–™çš„æ¯”ç”µå®¹å€¼
2. **ç‰¹å¾é‡è¦æ€§åˆ†æ**ï¼šåˆ†æä¸åŒå®éªŒå‚æ•°å¯¹ææ–™æ€§èƒ½çš„å½±å“æœºåˆ¶
3. **å½¢è²Œåˆ†ç±»é¢„æµ‹**ï¼šé¢„æµ‹åˆæˆæ¡ä»¶å¯¹ææ–™å½¢è²Œï¼ˆ1Dã€2Dã€3Dï¼‰çš„å½±å“

#### 1.2.2 æŠ€æœ¯ç›®æ ‡
- **é¢„æµ‹ç²¾åº¦**ï¼šRÂ² > 0.90ï¼ŒRMSE < 150 F/g
- **åˆ†ç±»å‡†ç¡®ç‡**ï¼š> 85%
- **ç‰¹å¾é€‰æ‹©**ï¼šè¯†åˆ«å…³é”®å½±å“å› ç´ 
- **å¯è§£é‡Šæ€§**ï¼šæä¾›æ¨¡å‹å†³ç­–ä¾æ®

#### 1.2.3 åº”ç”¨ç›®æ ‡
- **å®éªŒæŒ‡å¯¼**ï¼šä¸ºæ–°ææ–™åˆæˆæä¾›å‚æ•°å»ºè®®
- **æ€§èƒ½ä¼˜åŒ–**ï¼šæŒ‡å¯¼ææ–™æ€§èƒ½æå‡
- **æˆæœ¬æ§åˆ¶**ï¼šå‡å°‘è¯•é”™å®éªŒæˆæœ¬
- **çŸ¥è¯†ç§¯ç´¯**ï¼šå»ºç«‹ææ–™-æ€§èƒ½å…³ç³»æ•°æ®åº“

## äºŒã€æ•°æ®é›†

### 2.1 æ•°æ®é›†æ¦‚è¿°

#### 2.1.1 ä¸»æ•°æ®é›†ï¼ˆdata_aftercalculate.xlsxï¼‰

**æ•°æ®åŸºæœ¬ä¿¡æ¯**ï¼š
- **æ•°æ®è§„æ¨¡**ï¼š351æ¡è®°å½•ï¼Œ15ä¸ªç‰¹å¾
- **æ•°æ®æ¥æº**ï¼šä»å·²å‘è¡¨è®ºæ–‡ä¸­æå–çš„å®éªŒæ•°æ®
- **æ•°æ®è´¨é‡**ï¼šå®Œæ•´æ€§ > 95%ï¼Œå‡†ç¡®æ€§ç»è¿‡ä¸“å®¶éªŒè¯
- **æ—¶é—´è·¨åº¦**ï¼š2020-2024å¹´æœ€æ–°ç ”ç©¶æ•°æ®

**ç‰¹å¾è¯¦ç»†è¯´æ˜**ï¼š

| ç‰¹å¾ç±»åˆ« | ç‰¹å¾åç§° | æ•°æ®ç±»å‹ | å•ä½ | å–å€¼èŒƒå›´ | è¯´æ˜ |
|---------|---------|---------|------|---------|------|
| **å®éªŒå‚æ•°** | r-time | æ•°å€¼å‹ | h | 2-24 | ååº”æ—¶é—´ |
| | r-temperature | æ•°å€¼å‹ | Â°C | 120-200 | ååº”æ¸©åº¦ |
| | c-time | æ•°å€¼å‹ | h | 1-6 | ç……çƒ§æ—¶é—´ |
| | c-temperature | æ•°å€¼å‹ | Â°C | 300-600 | ç……çƒ§æ¸©åº¦ |
| **ææ–™å‚æ•°** | Co3+ | æ•°å€¼å‹ | mmol | 0.5-4.0 | Co3+æµ“åº¦ |
| | urea | æ•°å€¼å‹ | mmol | 2-20 | å°¿ç´ æµ“åº¦ |
| | NH4F | æ•°å€¼å‹ | mmol | 0-15 | NH4Fæµ“åº¦ |
| | SDS | æ•°å€¼å‹ | mmol | 0-10 | SDSæµ“åº¦ |
| | load mass-mg/cm2 | æ•°å€¼å‹ | mg/cmÂ² | 0.5-3.0 | è´Ÿè½½è´¨é‡ |
| **å…¶ä»–å‚æ•°** | Co-material | åˆ†ç±»å‹ | - | 3ç§ | é’´æºææ–™ |
| | F-base | åˆ†ç±»å‹ | - | 6ç§ | åŸºåº•ç±»å‹ |
| | code | åˆ†ç±»å‹ | - | 6ç§ | ææ–™ä»£ç  |
| **ç›®æ ‡å˜é‡** | Secondary Morphology | åˆ†ç±»å‹ | - | 40ç§ | å½¢è²Œç±»å‹ |

**æ•°æ®åˆ†å¸ƒç‰¹å¾**ï¼š
- **æ•°å€¼ç‰¹å¾**ï¼šæ­£æ€åˆ†å¸ƒï¼Œæ— æ˜æ˜¾åæ–œ
- **åˆ†ç±»ç‰¹å¾**ï¼šç±»åˆ«åˆ†å¸ƒç›¸å¯¹å‡åŒ€
- **ç›®æ ‡å˜é‡**ï¼š40ä¸ªå½¢è²Œç±»åˆ«ï¼Œåˆ†å¸ƒè¾ƒä¸ºå¹³è¡¡

#### 2.1.2 æ–‡çŒ®æ¥æºæ•°æ®é›†ï¼ˆæ•°æ®é›†æ¥æºæ–‡çŒ®.xlsxï¼‰

**æ•°æ®é›†ä¿¡æ¯**ï¼š
- **æ•°æ®è§„æ¨¡**ï¼š74æ¡è®°å½•
- **å†…å®¹**ï¼šç›¸å…³ç ”ç©¶è®ºæ–‡çš„DOIä¿¡æ¯
- **ç”¨é€”**ï¼šæ•°æ®æº¯æºå’ŒéªŒè¯
- **è¦†ç›–æœŸåˆŠ**ï¼šNature, Science, Advanced Materialsç­‰é¡¶çº§æœŸåˆŠ

**æ•°æ®éªŒè¯**ï¼š
- **æ¥æºå¯é æ€§**ï¼šæ¥è‡ªSCIæœŸåˆŠå‘è¡¨è®ºæ–‡
- **å®éªŒå¯é‡å¤æ€§**ï¼šè¯¦ç»†çš„å®éªŒæ¡ä»¶è®°å½•
- **æ•°æ®ä¸€è‡´æ€§**ï¼šç»Ÿä¸€çš„æµ‹è¯•æ ‡å‡†å’Œæ–¹æ³•

### 2.2 æ•°æ®é¢„å¤„ç†è¯¦ç»†è¯´æ˜

#### 2.2.1 æ•°æ®æ¸…æ´—ç­–ç•¥

**ç¼ºå¤±å€¼å¤„ç†**ï¼š
```python
# ç¼ºå¤±å€¼ç»Ÿè®¡
missing_counts = df.isnull().sum()
print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:\n{missing_counts[missing_counts > 0]}")

# å¤„ç†ç­–ç•¥
# 1. æ•°å€¼å‹ç‰¹å¾ï¼šç”¨ä¸­ä½æ•°å¡«å……
df['c-time'].fillna(df['c-time'].median(), inplace=True)
df['c-temperature'].fillna(df['c-temperature'].median(), inplace=True)
df['load mass-mg/cm2'].fillna(df['load mass-mg/cm2'].median(), inplace=True)

# 2. åˆ†ç±»ç‰¹å¾ï¼šç”¨ä¼—æ•°å¡«å……
df['Co-material'].fillna(df['Co-material'].mode()[0], inplace=True)
```

**å¼‚å¸¸å€¼å¤„ç†**ï¼š
```python
# ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
def detect_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
```

#### 2.2.2 ç‰¹å¾å·¥ç¨‹

**åˆ†ç±»ç‰¹å¾ç¼–ç **ï¼š
```python
from sklearn.preprocessing import LabelEncoder

# åˆ›å»ºç¼–ç å™¨å­—å…¸
encoders = {}

# å¯¹æ¯ä¸ªåˆ†ç±»ç‰¹å¾è¿›è¡Œç¼–ç 
categorical_features = ['Co-material', 'F-base', 'code', 'Secondary Morphology']
for feature in categorical_features:
    le = LabelEncoder()
    df[f'{feature}_encoded'] = le.fit_transform(df[feature])
    encoders[feature] = le
    print(f"åˆ— '{feature}' å·²ç¼–ç ä¸º '{feature}_encoded'")
    print(f"  å”¯ä¸€å€¼: {df[feature].unique()}")
```

**æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–**ï¼š
```python
from sklearn.preprocessing import StandardScaler

# é€‰æ‹©æ•°å€¼ç‰¹å¾
numeric_features = ['r-time', 'r-temperature', 'c-time', 'c-temperature', 
                   'Co3+', 'urea', 'NH4F', 'SDS', 'load mass-mg/cm2']

# æ ‡å‡†åŒ–
scaler = StandardScaler()
df[numeric_features] = scaler.fit_transform(df[numeric_features])
```

#### 2.2.3 æ•°æ®è´¨é‡è¯„ä¼°

**æ•°æ®å®Œæ•´æ€§æ£€æŸ¥**ï¼š
- ç¼ºå¤±å€¼æ¯”ä¾‹ < 5%
- é‡å¤å€¼æ£€æŸ¥
- æ•°æ®ç±»å‹ä¸€è‡´æ€§

**æ•°æ®åˆ†å¸ƒæ£€æŸ¥**ï¼š
- ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–
- å¼‚å¸¸å€¼æ£€æµ‹
- ç›¸å…³æ€§åˆ†æ

**ç›®æ ‡å˜é‡åˆ†æ**ï¼š
- ç±»åˆ«åˆ†å¸ƒå¹³è¡¡æ€§
- ç¨€æœ‰ç±»åˆ«å¤„ç†
- æ ‡ç­¾è´¨é‡éªŒè¯

## ä¸‰ã€é¡¹ç›®ç»“æ„

### 3.1 ç›®å½•ç»“æ„

```
é¡¹ç›®ä¸€/
â”œâ”€â”€ data/                    # æ•°æ®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ data_aftercalculate.xlsx    # ä¸»æ•°æ®é›†ï¼ˆ351æ¡è®°å½•ï¼Œ15ä¸ªç‰¹å¾ï¼‰
â”‚   â””â”€â”€ æ•°æ®é›†æ¥æºæ–‡çŒ®.xlsx         # æ–‡çŒ®æ¥æºæ•°æ®ï¼ˆ74æ¡è®°å½•ï¼‰
â”‚
â”œâ”€â”€ src/                     # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_preprocessing.py   # æ•°æ®é¢„å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ feature_selection.py    # ç‰¹å¾é€‰æ‹©æ¨¡å—
â”‚   â”œâ”€â”€ model_training.py       # æ¨¡å‹è®­ç»ƒæ¨¡å—
â”‚   â”œâ”€â”€ shap_analysis.py        # SHAPåˆ†ææ¨¡å—
â”‚   â””â”€â”€ font_config.py          # å­—ä½“é…ç½®
â”‚
â”œâ”€â”€ models/                  # è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶
â”‚   â”œâ”€â”€ DT_model.pkl            # å†³ç­–æ ‘æ¨¡å‹
â”‚   â”œâ”€â”€ RF_model.pkl            # éšæœºæ£®æ—æ¨¡å‹
â”‚   â”œâ”€â”€ RF_Tuned_model.pkl      # è°ƒä¼˜åçš„éšæœºæ£®æ—æ¨¡å‹
â”‚   â”œâ”€â”€ XGB_model.pkl           # XGBoostæ¨¡å‹
â”‚   â””â”€â”€ Stacking_model.pkl      # å †å æ¨¡å‹
â”‚
â”œâ”€â”€ plots/                   # å¯è§†åŒ–å›¾è¡¨
â”‚   â””â”€â”€ feature_ranking.png     # ç‰¹å¾é‡è¦æ€§æ’åºå›¾
â”‚
â”œâ”€â”€ results/                 # åˆ†æç»“æœæ–‡ä»¶
â”‚   â”œâ”€â”€ feature_selection_results.xlsx  # ç‰¹å¾é€‰æ‹©ç»“æœ
â”‚   â”œâ”€â”€ training_results.xlsx           # è®­ç»ƒç»“æœ
â”‚   â””â”€â”€ shap_analysis_results.xlsx      # SHAPåˆ†æç»“æœ
â”‚
â”œâ”€â”€ docs/                    # é¡¹ç›®æ–‡æ¡£
â”‚   â”œâ”€â”€ é¡¹ç›®è¯¦ç»†è¯´æ˜æ–‡æ¡£.md
â”‚   â”œâ”€â”€ é¡¹ç›®æ–‡ä»¶ç»“æ„è¯´æ˜.md
â”‚   â””â”€â”€ æ¨¡å‹æ–‡ä»¶ä½¿ç”¨æŒ‡å—.md
â”‚
â”œâ”€â”€ scripts/                 # å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ check_data_types.py     # æ•°æ®ç±»å‹æ£€æŸ¥
â”‚   â””â”€â”€ check_target_distribution.py  # ç›®æ ‡åˆ†å¸ƒæ£€æŸ¥
â”‚
â”œâ”€â”€ tests/                   # æµ‹è¯•æ–‡ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_data_preprocessing.py
â”‚
â”œâ”€â”€ main.py                     # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ config.py                   # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt            # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ setup.py                    # å®‰è£…é…ç½®
â”œâ”€â”€ pyproject.toml              # ç°ä»£Pythoné¡¹ç›®é…ç½®
â”œâ”€â”€ Makefile                    # æ„å»ºè„šæœ¬
â”œâ”€â”€ .gitignore                  # Gitå¿½ç•¥æ–‡ä»¶
â””â”€â”€ README.md                   # é¡¹ç›®è¯´æ˜
```

### 3.2 é…ç½®æ–‡ä»¶è¯´æ˜

#### 3.2.1 `config.py`
- **æ•°æ®é›†è·¯å¾„é…ç½®**ï¼šç®¡ç†æ•°æ®æ–‡ä»¶è·¯å¾„
- **æ¨¡å‹å‚æ•°é…ç½®**ï¼šè®¾ç½®éšæœºç§å­ã€æµ‹è¯•é›†æ¯”ä¾‹ç­‰
- **ç‰¹å¾é€‰æ‹©é…ç½®**ï¼šæŒ‡å®šç‰¹å¾é€‰æ‹©å‚æ•°
- **è¾“å‡ºç›®å½•é…ç½®**ï¼šç®¡ç†ç»“æœæ–‡ä»¶è¾“å‡ºè·¯å¾„
- **è¶…å‚æ•°è°ƒä¼˜é…ç½®**ï¼šå®šä¹‰å„æ¨¡å‹çš„è¶…å‚æ•°æœç´¢ç©ºé—´

#### 3.2.2 `requirements.txt`
- **é¡¹ç›®ä¾èµ–åŒ…åˆ—è¡¨**ï¼šæŒ‡å®šæ‰€æœ‰å¿…éœ€çš„PythonåŒ…
- **ç‰ˆæœ¬è¦æ±‚**ï¼šç¡®ä¿ç¯å¢ƒå…¼å®¹æ€§

#### 3.2.3 `setup.py` & `pyproject.toml`
- **é¡¹ç›®å®‰è£…é…ç½®**ï¼šæ”¯æŒpipå®‰è£…
- **å¼€å‘ä¾èµ–é…ç½®**ï¼šæµ‹è¯•å’Œä»£ç è´¨é‡å·¥å…·
- **ä»£ç è´¨é‡å·¥å…·é…ç½®**ï¼šblackã€flake8ã€mypyç­‰

#### 3.2.4 `Makefile`
- **å¸¸ç”¨å‘½ä»¤å¿«æ·æ–¹å¼**ï¼šç®€åŒ–é¡¹ç›®æ“ä½œ
- **é¡¹ç›®æ„å»ºå’Œæµ‹è¯•å‘½ä»¤**ï¼šæ ‡å‡†åŒ–å¼€å‘æµç¨‹

### 3.3 æ•°æ®æµå‘

```
åŸå§‹æ•°æ® (data/) 
    â†“
æ•°æ®é¢„å¤„ç† (src/data_preprocessing.py)
    â†“
ç‰¹å¾é€‰æ‹© (src/feature_selection.py)
    â†“
æ¨¡å‹è®­ç»ƒ (src/model_training.py)
    â†“
SHAPåˆ†æ (src/shap_analysis.py)
    â†“
ç»“æœè¾“å‡º (results/, models/, plots/)
```

### 3.4 ä½¿ç”¨æ–¹å¼

#### 3.4.1 å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
# æˆ–è€…
make install
```

#### 3.4.2 è¿è¡Œå®Œæ•´æµç¨‹
```bash
python main.py
# æˆ–è€…
make run
```

#### 3.4.3 äº¤äº’å¼è¿è¡Œ
```bash
python main.py --interactive
# æˆ–è€…
make run-interactive
```

#### 3.4.4 ä»£ç è´¨é‡æ£€æŸ¥
```bash
make lint      # ä»£ç æ£€æŸ¥
make format    # ä»£ç æ ¼å¼åŒ–
make test      # è¿è¡Œæµ‹è¯•
```

### 3.5 æ–‡ä»¶å‘½åè§„èŒƒ

- **Pythonæ–‡ä»¶**: ä½¿ç”¨ä¸‹åˆ’çº¿å‘½åæ³• (snake_case)
- **æ•°æ®æ–‡ä»¶**: ä½¿ç”¨æè¿°æ€§åç§°ï¼Œæ”¯æŒä¸­æ–‡
- **ç»“æœæ–‡ä»¶**: ä½¿ç”¨åŠŸèƒ½æè¿° + `_results.xlsx` æ ¼å¼
- **æ¨¡å‹æ–‡ä»¶**: ä½¿ç”¨æ¨¡å‹ç±»å‹ + `_model.pkl` æ ¼å¼
- **å›¾è¡¨æ–‡ä»¶**: ä½¿ç”¨åŠŸèƒ½æè¿° + `.png` æ ¼å¼

### 3.6 ç»“æœæ–‡ä»¶è¯´æ˜

#### 3.6.1 æ¨¡å‹æ–‡ä»¶ (`models/`)
- è®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹
- å¯ç”¨äºé¢„æµ‹æ–°æ•°æ®
- æ”¯æŒæ¨¡å‹åŠ è½½å’Œæ¨ç†

#### 3.6.2 ç»“æœæ–‡ä»¶ (`results/`)
- ç‰¹å¾é€‰æ‹©ç»“æœï¼šåŒ…å«ç‰¹å¾é‡è¦æ€§æ’åº
- æ¨¡å‹è®­ç»ƒæ€§èƒ½æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ç­‰
- SHAPåˆ†æç»“æœï¼šç‰¹å¾è´¡çŒ®åº¦åˆ†æ

#### 3.6.3 å›¾è¡¨æ–‡ä»¶ (`plots/`)
- ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
- æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾
- SHAPè§£é‡Šæ€§å›¾è¡¨

## å››ã€æ¨¡å‹ä»‹ç»

### 4.1 æ ¸å¿ƒç®—æ³•åŸç†

#### 4.1.1 é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰

**ç®—æ³•æ¦‚è¿°**ï¼š
RFEæ˜¯ä¸€ç§ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡é€’å½’åœ°è®­ç»ƒæ¨¡å‹å¹¶ç§»é™¤æœ€ä¸é‡è¦çš„ç‰¹å¾æ¥æ‰¾åˆ°æœ€ä¼˜ç‰¹å¾å­é›†ã€‚

**å·¥ä½œåŸç†**ï¼š
```python
def recursive_feature_elimination(X, y, estimator, n_features_to_select):
    """
    RFEç®—æ³•å®ç°
    """
    # 1. è®­ç»ƒåˆå§‹æ¨¡å‹
    estimator.fit(X, y)
    
    # 2. è®¡ç®—ç‰¹å¾é‡è¦æ€§
    feature_importance = estimator.feature_importances_
    
    # 3. ç§»é™¤æœ€ä¸é‡è¦çš„ç‰¹å¾
    while X.shape[1] > n_features_to_select:
        # æ‰¾åˆ°æœ€ä¸é‡è¦çš„ç‰¹å¾
        least_important = np.argmin(feature_importance)
        
        # ç§»é™¤è¯¥ç‰¹å¾
        X = np.delete(X, least_important, axis=1)
        feature_importance = np.delete(feature_importance, least_important)
        
        # é‡æ–°è®­ç»ƒæ¨¡å‹
        estimator.fit(X, y)
        feature_importance = estimator.feature_importances_
    
    return X, estimator
```

**ç®—æ³•ä¼˜åŠ¿**ï¼š
- **è‡ªåŠ¨ç‰¹å¾é€‰æ‹©**ï¼šæ— éœ€äººå·¥å¹²é¢„
- **æ¨¡å‹é©±åŠ¨**ï¼šåŸºäºæ¨¡å‹æ€§èƒ½é€‰æ‹©ç‰¹å¾
- **å¯è§£é‡Šæ€§å¼º**ï¼šæä¾›ç‰¹å¾é‡è¦æ€§æ’åº
- **è®¡ç®—æ•ˆç‡é«˜**ï¼šé€‚ç”¨äºä¸­ç­‰è§„æ¨¡æ•°æ®é›†

#### 4.1.2 éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰

**ç®—æ³•æ¦‚è¿°**ï¼š
éšæœºæ£®æ—æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå¤šä¸ªå†³ç­–æ ‘å¹¶å–å…¶å¹³å‡é¢„æµ‹ç»“æœã€‚

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š
```python
class RandomForest:
    def __init__(self, n_estimators=100, max_depth=None, random_state=42):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.random_state = random_state
        self.trees = []
    
    def fit(self, X, y):
        for i in range(self.n_estimators):
            # 1. Bootstrapé‡‡æ ·
            indices = np.random.choice(len(X), len(X), replace=True)
            X_bootstrap = X[indices]
            y_bootstrap = y[indices]
            
            # 2. æ„å»ºå†³ç­–æ ‘
            tree = DecisionTree(max_depth=self.max_depth)
            tree.fit(X_bootstrap, y_bootstrap)
            self.trees.append(tree)
    
    def predict(self, X):
        # 3. é›†æˆé¢„æµ‹
        predictions = [tree.predict(X) for tree in self.trees]
        return np.mean(predictions, axis=0)
```

**ç®—æ³•ä¼˜åŠ¿**ï¼š
- **æŠ—è¿‡æ‹Ÿåˆ**ï¼šå¤šæ ‘é›†æˆå‡å°‘è¿‡æ‹Ÿåˆé£é™©
- **éšæœºæ€§**ï¼šBootstrapé‡‡æ ·å’Œç‰¹å¾éšæœºé€‰æ‹©
- **ç‰¹å¾é‡è¦æ€§**ï¼šå†…ç½®ç‰¹å¾é‡è¦æ€§è¯„ä¼°
- **å‚æ•°é²æ£’**ï¼šå¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ

#### 4.1.3 æç«¯æ¢¯åº¦æå‡ï¼ˆXGBoostï¼‰

**ç®—æ³•æ¦‚è¿°**ï¼š
XGBoostæ˜¯ä¸€ç§é«˜æ•ˆçš„æ¢¯åº¦æå‡æ ‘ç®—æ³•ï¼Œé€šè¿‡ä¼˜åŒ–ç›®æ ‡å‡½æ•°æ¥æ„å»ºå¼ºå­¦ä¹ å™¨ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
```python
# XGBoostç›®æ ‡å‡½æ•°
def xgboost_objective(y_true, y_pred):
    """
    è‡ªå®šä¹‰ç›®æ ‡å‡½æ•°ç¤ºä¾‹
    """
    # å›å½’ä»»åŠ¡ï¼šå‡æ–¹è¯¯å·® + æ­£åˆ™åŒ–
    def reg_obj(predt, dtrain):
        y = dtrain.get_label()
        grad = 2 * (predt - y)  # ä¸€é˜¶å¯¼æ•°
        hess = 2 * np.ones_like(y)  # äºŒé˜¶å¯¼æ•°
        return grad, hess
    
    return reg_obj
```

**æŠ€æœ¯ç‰¹ç‚¹**ï¼š
- **æ¢¯åº¦æå‡**ï¼šé€æ£µæ ‘ä¼˜åŒ–ç›®æ ‡å‡½æ•°
- **æ­£åˆ™åŒ–**ï¼šL1/L2æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
- **ç¼ºå¤±å€¼å¤„ç†**ï¼šè‡ªåŠ¨å¤„ç†ç¼ºå¤±å€¼
- **å¹¶è¡Œè®¡ç®—**ï¼šæ”¯æŒå¤šçº¿ç¨‹è®­ç»ƒ
- **æ—©åœæœºåˆ¶**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ

#### 3.1.4 Stackingèåˆæ¨¡å‹

**ç®—æ³•æ¦‚è¿°**ï¼š
Stackingæ˜¯ä¸€ç§æ¨¡å‹èåˆæŠ€æœ¯ï¼Œé€šè¿‡è®­ç»ƒå…ƒå­¦ä¹ å™¨æ¥ç»„åˆå¤šä¸ªåŸºæ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

**å·¥ä½œæµç¨‹**ï¼š
```python
class StackingModel:
    def __init__(self, base_models, meta_model, cv_folds=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.cv_folds = cv_folds
    
    def fit(self, X, y):
        # 1. è®­ç»ƒåŸºæ¨¡å‹
        base_predictions = []
        for model in self.base_models:
            # äº¤å‰éªŒè¯é¢„æµ‹
            cv_preds = cross_val_predict(model, X, y, cv=self.cv_folds)
            base_predictions.append(cv_preds)
        
        # 2. è®­ç»ƒå…ƒå­¦ä¹ å™¨
        meta_features = np.column_stack(base_predictions)
        self.meta_model.fit(meta_features, y)
        
        # 3. è®­ç»ƒæœ€ç»ˆåŸºæ¨¡å‹
        for model in self.base_models:
            model.fit(X, y)
    
    def predict(self, X):
        # 4. ç”Ÿæˆå…ƒç‰¹å¾
        base_preds = [model.predict(X) for model in self.base_models]
        meta_features = np.column_stack(base_preds)
        
        # 5. å…ƒå­¦ä¹ å™¨é¢„æµ‹
        return self.meta_model.predict(meta_features)
```

**èåˆä¼˜åŠ¿**ï¼š
- **æ¨¡å‹äº’è¡¥**ï¼šä¸åŒæ¨¡å‹æ•æ‰ä¸åŒæ¨¡å¼
- **æ€§èƒ½æå‡**ï¼šé€šå¸¸ä¼˜äºå•ä¸ªæ¨¡å‹
- **é²æ£’æ€§å¼º**ï¼šå‡å°‘å•ä¸ªæ¨¡å‹åå·®
- **çµæ´»æ€§é«˜**ï¼šå¯ç»„åˆä»»æ„æ¨¡å‹

### 4.2 æ¨¡å‹æ¶æ„è®¾è®¡

#### 4.2.1 ä¹æ¨¡å‹ç»„åˆæ¶æ„

**æ¨¡å‹ç»„åˆç­–ç•¥**ï¼š
```
ç‰¹å¾é€‰æ‹©å™¨ Ã— é¢„æµ‹æ¨¡å‹ = 9ç§ç»„åˆ

ç‰¹å¾é€‰æ‹©å™¨ï¼š
â”œâ”€â”€ RF (Random Forest)
â”œâ”€â”€ XGB (XGBoost)  
â””â”€â”€ RT (Decision Tree)

é¢„æµ‹æ¨¡å‹ï¼š
â”œâ”€â”€ RF (Random Forest)
â”œâ”€â”€ XGB (XGBoost)
â””â”€â”€ RT (Decision Tree)

ç»„åˆç»“æœï¼š
â”œâ”€â”€ RF-RFE-RF     # éšæœºæ£®æ—ç‰¹å¾é€‰æ‹© + éšæœºæ£®æ—é¢„æµ‹
â”œâ”€â”€ RF-RFE-XGB    # éšæœºæ£®æ—ç‰¹å¾é€‰æ‹© + XGBoosté¢„æµ‹
â”œâ”€â”€ RF-RFE-RT     # éšæœºæ£®æ—ç‰¹å¾é€‰æ‹© + å†³ç­–æ ‘é¢„æµ‹
â”œâ”€â”€ XGB-RFE-RF    # XGBoostç‰¹å¾é€‰æ‹© + éšæœºæ£®æ—é¢„æµ‹
â”œâ”€â”€ XGB-RFE-XGB   # XGBoostç‰¹å¾é€‰æ‹© + XGBoosté¢„æµ‹ï¼ˆæœ€ä¼˜ï¼‰
â”œâ”€â”€ XGB-RFE-RT    # XGBoostç‰¹å¾é€‰æ‹© + å†³ç­–æ ‘é¢„æµ‹
â”œâ”€â”€ RT-RFE-RF     # å†³ç­–æ ‘ç‰¹å¾é€‰æ‹© + éšæœºæ£®æ—é¢„æµ‹
â”œâ”€â”€ RT-RFE-XGB    # å†³ç­–æ ‘ç‰¹å¾é€‰æ‹© + XGBoosté¢„æµ‹
â””â”€â”€ RT-RFE-RT     # å†³ç­–æ ‘ç‰¹å¾é€‰æ‹© + å†³ç­–æ ‘é¢„æµ‹
```

#### 4.2.2 æ¨¡å‹é€‰æ‹©ç­–ç•¥

**æ€§èƒ½è¯„ä¼°æŒ‡æ ‡**ï¼š
- **å›å½’ä»»åŠ¡**ï¼šRÂ², RMSE, MAE
- **åˆ†ç±»ä»»åŠ¡**ï¼šå‡†ç¡®ç‡, ç²¾ç¡®ç‡, å¬å›ç‡, F1åˆ†æ•°
- **ç‰¹å¾é€‰æ‹©**ï¼šç‰¹å¾é‡è¦æ€§æ’åº
- **æ¨¡å‹ç¨³å®šæ€§**ï¼šäº¤å‰éªŒè¯æ ‡å‡†å·®

**æœ€ä¼˜æ¨¡å‹é€‰æ‹©**ï¼š
```python
def select_best_model(results):
    """
    é€‰æ‹©æœ€ä¼˜æ¨¡å‹
    """
    # ç»¼åˆè¯„åˆ†è®¡ç®—
    for model_name, metrics in results.items():
        # å›å½’è¯„åˆ†
        regression_score = metrics['R2'] * 0.4 + (1 - metrics['RMSE_norm']) * 0.3 + (1 - metrics['MAE_norm']) * 0.3
        
        # åˆ†ç±»è¯„åˆ†
        classification_score = metrics['accuracy'] * 0.4 + metrics['precision'] * 0.2 + metrics['recall'] * 0.2 + metrics['f1'] * 0.2
        
        # ç»¼åˆè¯„åˆ†
        total_score = regression_score * 0.6 + classification_score * 0.4
        results[model_name]['total_score'] = total_score
    
    # é€‰æ‹©æœ€é«˜åˆ†æ¨¡å‹
    best_model = max(results.items(), key=lambda x: x[1]['total_score'])
    return best_model[0]
```

#### 4.2.3 è¶…å‚æ•°ä¼˜åŒ–

**ç½‘æ ¼æœç´¢ç­–ç•¥**ï¼š
```python
# éšæœºæ£®æ—è¶…å‚æ•°
rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# XGBoostè¶…å‚æ•°
xgb_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}
```

**è´å¶æ–¯ä¼˜åŒ–**ï¼š
```python
from skopt import BayesSearchCV

# è´å¶æ–¯ä¼˜åŒ–
bayes_search = BayesSearchCV(
    estimator=XGBRegressor(),
    search_spaces=xgb_params,
    n_iter=50,
    cv=5,
    scoring='r2',
    random_state=42
)
```

## äº”ã€éƒ¨ç½²æŒ‡å—

### 5.1 ç³»ç»Ÿè¦æ±‚

#### 5.1.1 ç¡¬ä»¶è¦æ±‚

**æœ€ä½é…ç½®**ï¼š
- **CPU**ï¼šIntel i5æˆ–AMD Ryzen 5ï¼ˆ4æ ¸å¿ƒä»¥ä¸Šï¼‰
- **å†…å­˜**ï¼š8GB RAM
- **å­˜å‚¨**ï¼š20GBå¯ç”¨ç©ºé—´
- **æ˜¾å¡**ï¼šé›†æˆæ˜¾å¡å³å¯ï¼ˆæ— éœ€ç‹¬ç«‹æ˜¾å¡ï¼‰
- **ç½‘ç»œ**ï¼šç¨³å®šçš„äº’è”ç½‘è¿æ¥ï¼ˆç”¨äºä¸‹è½½ä¾èµ–åŒ…ï¼‰

**æ¨èé…ç½®**ï¼š
- **CPU**ï¼šIntel i7æˆ–AMD Ryzen 7ï¼ˆ8æ ¸å¿ƒä»¥ä¸Šï¼‰
- **å†…å­˜**ï¼š16GB RAM
- **å­˜å‚¨**ï¼š50GBå¯ç”¨ç©ºé—´ï¼ˆSSDæ¨èï¼‰
- **æ˜¾å¡**ï¼šNVIDIA GTX 1060æˆ–æ›´é«˜ï¼ˆå¯é€‰ï¼Œç”¨äºGPUåŠ é€Ÿï¼‰
- **ç½‘ç»œ**ï¼šé«˜é€Ÿäº’è”ç½‘è¿æ¥

#### 5.1.2 è½¯ä»¶è¦æ±‚

**æ“ä½œç³»ç»Ÿ**ï¼š
- **Windows**ï¼šWindows 10/11ï¼ˆ64ä½ï¼‰
- **macOS**ï¼šmacOS 10.14+ï¼ˆMojaveåŠä»¥ä¸Šï¼‰
- **Linux**ï¼šUbuntu 18.04+, CentOS 7+, RHEL 7+

**Pythonç¯å¢ƒ**ï¼š
- **Pythonç‰ˆæœ¬**ï¼š3.8-3.11ï¼ˆæ¨è3.9ï¼‰
- **åŒ…ç®¡ç†å™¨**ï¼špip 20.0+ æˆ– conda 4.8+
- **è™šæ‹Ÿç¯å¢ƒ**ï¼šæ¨èä½¿ç”¨è™šæ‹Ÿç¯å¢ƒéš”ç¦»é¡¹ç›®ä¾èµ–

### 5.2 ç¯å¢ƒéƒ¨ç½²è¯¦ç»†æ­¥éª¤

#### 5.2.1 æ–¹æ³•ä¸€ï¼šAnacondaç¯å¢ƒéƒ¨ç½²ï¼ˆæ¨èï¼‰

**æ­¥éª¤1ï¼šä¸‹è½½å®‰è£…Anaconda**
```bash
# 1. è®¿é—®å®˜ç½‘ä¸‹è½½
# https://www.anaconda.com/download

# 2. é€‰æ‹©é€‚åˆçš„ç‰ˆæœ¬
# - Windows: Anaconda3-2023.09-Windows-x86_64.exe
# - macOS: Anaconda3-2023.09-MacOSX-x86_64.pkg
# - Linux: Anaconda3-2023.09-Linux-x86_64.sh

# 3. å®‰è£…é€‰é¡¹
# - é€‰æ‹©"Install for All Users"ï¼ˆæ¨èï¼‰
# - å‹¾é€‰"Add Anaconda to PATH"
# - å‹¾é€‰"Register Anaconda as default Python"
```

**æ­¥éª¤2ï¼šåˆ›å»ºé¡¹ç›®ç¯å¢ƒ**
```bash
# æ‰“å¼€Anaconda Promptæˆ–ç»ˆç«¯
conda create -n mgco2o4_ml python=3.9
conda activate mgco2o4_ml

# éªŒè¯ç¯å¢ƒ
python --version  # åº”è¯¥æ˜¾ç¤ºPython 3.9.x
conda info --envs  # æŸ¥çœ‹æ‰€æœ‰ç¯å¢ƒ
```

**æ­¥éª¤3ï¼šå®‰è£…é¡¹ç›®ä¾èµ–**
```bash
# åŸºç¡€ç§‘å­¦è®¡ç®—åŒ…
conda install -c conda-forge pandas=1.5.3 numpy=1.24.3 matplotlib=3.7.1 seaborn=0.12.2

# æœºå™¨å­¦ä¹ åŒ…
conda install -c conda-forge scikit-learn=1.3.0
pip install xgboost==1.7.6

# æ•°æ®å¯è§†åŒ–åŒ…
pip install shap==0.42.1 plotly==5.15.0

# Excelæ–‡ä»¶å¤„ç†
pip install openpyxl==3.1.2

# å¯é€‰ï¼šJupyter notebook
conda install jupyter notebook
```

**æ­¥éª¤4ï¼šéªŒè¯å®‰è£…**
```bash
# åˆ›å»ºæµ‹è¯•è„šæœ¬
python -c "
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import xgboost as xgb
import shap
import plotly.express as px
import openpyxl

print('æ‰€æœ‰åŒ…å®‰è£…æˆåŠŸï¼')
print(f'pandas: {pd.__version__}')
print(f'numpy: {np.__version__}')
print(f'scikit-learn: {sklearn.__version__}')
print(f'xgboost: {xgb.__version__}')
print(f'shap: {shap.__version__}')
"
```

#### 5.2.2 æ–¹æ³•äºŒï¼šPyCharmç¯å¢ƒéƒ¨ç½²

**æ­¥éª¤1ï¼šä¸‹è½½å®‰è£…PyCharm**
```bash
# 1. è®¿é—®å®˜ç½‘ä¸‹è½½
# https://www.jetbrains.com/pycharm/

# 2. é€‰æ‹©ç‰ˆæœ¬
# - Community Editionï¼ˆå…è´¹ï¼ŒåŠŸèƒ½è¶³å¤Ÿï¼‰
# - Professional Editionï¼ˆä»˜è´¹ï¼ŒåŠŸèƒ½æ›´å…¨ï¼‰

# 3. å®‰è£…é€‰é¡¹
# - å‹¾é€‰"Create Desktop Shortcut"
# - å‹¾é€‰"Update PATH variable"
# - å‹¾é€‰"Update Context Menu"
```

**æ­¥éª¤2ï¼šé…ç½®Pythonè§£é‡Šå™¨**
```bash
# 1. æ‰“å¼€PyCharm
# 2. é€‰æ‹©"File" â†’ "New Project"
# 3. é…ç½®è§£é‡Šå™¨ï¼š
#    - é€‰æ‹©"Previously configured interpreter"
#    - ç‚¹å‡»é½¿è½®å›¾æ ‡ â†’ "Add"
#    - é€‰æ‹©"Conda Environment" â†’ "Existing environment"
#    - é€‰æ‹©condaç¯å¢ƒè·¯å¾„ï¼š
#      Windows: C:\Users\ç”¨æˆ·å\anaconda3\envs\mgco2o4_ml\python.exe
#      macOS/Linux: /Users/ç”¨æˆ·å/anaconda3/envs/mgco2o4_ml/bin/python
```

**æ­¥éª¤3ï¼šåˆ›å»ºé¡¹ç›®ç»“æ„**
```bash
# åœ¨PyCharmä¸­åˆ›å»ºä»¥ä¸‹ç›®å½•ç»“æ„
mgco2o4_project/
â”œâ”€â”€ data/                    # æ•°æ®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ data_aftercalculate.xlsx
â”‚   â””â”€â”€ æ•°æ®é›†æ¥æºæ–‡çŒ®.xlsx
â”œâ”€â”€ src/                     # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ feature_selection.py
â”‚   â”œâ”€â”€ model_training.py
â”‚   â”œâ”€â”€ shap_analysis.py
â”‚   â””â”€â”€ font_config.py
â”œâ”€â”€ models/                  # æ¨¡å‹ä¿å­˜ç›®å½•
â”œâ”€â”€ results/                 # ç»“æœè¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ plots/
â”‚   â””â”€â”€ reports/
â”œâ”€â”€ tests/                   # æµ‹è¯•æ–‡ä»¶ç›®å½•
â”œâ”€â”€ requirements.txt         # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ config.py               # é…ç½®æ–‡ä»¶
â”œâ”€â”€ main.py                 # ä¸»ç¨‹åº
â””â”€â”€ README.md               # é¡¹ç›®è¯´æ˜
```





### 5.3 ç¯å¢ƒéªŒè¯

#### 5.3.1 åŸºç¡€åŠŸèƒ½æµ‹è¯•
```python
# test_environment.py
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import shap

def test_basic_functionality():
    """æµ‹è¯•åŸºç¡€åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹ç¯å¢ƒæµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    X = np.random.rand(100, 5)
    y = np.random.rand(100)
    
    # æµ‹è¯•æœºå™¨å­¦ä¹ æ¨¡å‹
    rf = RandomForestRegressor(n_estimators=10, random_state=42)
    rf.fit(X, y)
    rf_score = rf.score(X, y)
    
    xgb_model = xgb.XGBRegressor(n_estimators=10, random_state=42)
    xgb_model.fit(X, y)
    xgb_score = xgb_model.score(X, y)
    
    # æµ‹è¯•SHAP
    explainer = shap.TreeExplainer(rf)
    shap_values = explainer.shap_values(X[:10])
    
    print(f"éšæœºæ£®æ—æµ‹è¯•: {rf_score:.4f}")
print(f"XGBoostæµ‹è¯•: {xgb_score:.4f}")
print(f"SHAPæµ‹è¯•: å½¢çŠ¶ {shap_values.shape}")
    print("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")

if __name__ == "__main__":
    test_basic_functionality()
```

#### 5.3.2 æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
# benchmark_test.py
import time
import numpy as np
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

def benchmark_models():
    """æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    # åˆ›å»ºå¤§è§„æ¨¡æµ‹è¯•æ•°æ®
    X = np.random.rand(1000, 20)
    y = np.random.rand(1000)
    
    # æµ‹è¯•éšæœºæ£®æ—
    start_time = time.time()
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X, y)
    rf_time = time.time() - start_time
    
    # æµ‹è¯•XGBoost
    start_time = time.time()
    xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)
    xgb_model.fit(X, y)
    xgb_time = time.time() - start_time
    
    print(f"éšæœºæ£®æ—è®­ç»ƒæ—¶é—´: {rf_time:.2f}ç§’")
print(f"XGBoostè®­ç»ƒæ—¶é—´: {xgb_time:.2f}ç§’")
print(f"æ€§èƒ½æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    benchmark_models()
```

## å…­ã€è®­ç»ƒä¸æµ‹è¯•

### 6.1 æ•°æ®é¢„å¤„ç†è¯¦ç»†æ­¥éª¤

#### 6.1.1 æ•°æ®åŠ è½½ä¸æ¸…æ´—
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

def load_and_clean_data(data_path):
    """
    æ•°æ®åŠ è½½å’Œæ¸…æ´—å‡½æ•°
    """
    print("å¼€å§‹æ•°æ®åŠ è½½å’Œæ¸…æ´—...")
    
    # åŠ è½½æ•°æ®
    df = pd.read_excel(data_path)
    print(f"æ•°æ®åŠ è½½æˆåŠŸï¼æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # åˆ é™¤æ— ç”¨åˆ—
    if 'Unnamed: 0' in df.columns:
        df = df.drop('Unnamed: 0', axis=1)
        print("åˆ é™¤æ— ç”¨åˆ— 'Unnamed: 0'")
    
    # å¤„ç†ç¼ºå¤±å€¼
    missing_counts = df.isnull().sum()
    print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:\n{missing_counts[missing_counts > 0]}")
    
    # æ•°å€¼å‹ç‰¹å¾ç”¨ä¸­ä½æ•°å¡«å……
    numeric_features = ['c-time', 'c-temperature', 'load mass-mg/cm2']
    for col in numeric_features:
        if col in df.columns and df[col].isnull().sum() > 0:
            median_val = df[col].median()
            df[col].fillna(median_val, inplace=True)
            print(f"åˆ— '{col}' ç¼ºå¤±å€¼å·²ç”¨ä¸­ä½æ•° {median_val:.2f} å¡«å……")
    
    # åˆ†ç±»ç‰¹å¾ç”¨ä¼—æ•°å¡«å……
    categorical_features = ['Co-material', 'F-base', 'code']
    for col in categorical_features:
        if col in df.columns and df[col].isnull().sum() > 0:
            mode_val = df[col].mode()[0]
            df[col].fillna(mode_val, inplace=True)
            print(f"åˆ— '{col}' ç¼ºå¤±å€¼å·²ç”¨ä¼—æ•° '{mode_val}' å¡«å……")
    
    print("æ•°æ®æ¸…æ´—å®Œæˆï¼")
    return df

# ä½¿ç”¨ç¤ºä¾‹
df = load_and_clean_data('data/data_aftercalculate.xlsx')
```

#### 6.1.2 ç‰¹å¾å·¥ç¨‹
```python
def encode_categorical_features(df):
    """
    åˆ†ç±»ç‰¹å¾ç¼–ç å‡½æ•°
    """
    print("å¼€å§‹åˆ†ç±»ç‰¹å¾ç¼–ç ...")
    
    # åˆ›å»ºç¼–ç å™¨å­—å…¸
    encoders = {}
    
    # éœ€è¦ç¼–ç çš„åˆ†ç±»ç‰¹å¾
    categorical_features = ['Co-material', 'F-base', 'code', 'Secondary Morphology']
    
    for feature in categorical_features:
        if feature in df.columns:
            le = LabelEncoder()
            df[f'{feature}_encoded'] = le.fit_transform(df[feature])
            encoders[feature] = le
            
            # æ˜¾ç¤ºç¼–ç ä¿¡æ¯
            unique_values = df[feature].unique()
            print(f"åˆ— '{feature}' å·²ç¼–ç ä¸º '{feature}_encoded'")
            print(f"  å”¯ä¸€å€¼: {unique_values}")
            print(f"  ç¼–ç æ˜ å°„: {dict(zip(unique_values, range(len(unique_values))))}")
    
    print("åˆ†ç±»ç‰¹å¾ç¼–ç å®Œæˆï¼")
    return df, encoders

def standardize_numeric_features(df):
    """
    æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–å‡½æ•°
    """
    print("å¼€å§‹æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–...")
    
    # é€‰æ‹©æ•°å€¼ç‰¹å¾
    numeric_features = ['r-time', 'r-temperature', 'c-time', 'c-temperature', 
                       'Co3+', 'urea', 'NH4F', 'SDS', 'load mass-mg/cm2']
    
    # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
    available_features = [col for col in numeric_features if col in df.columns]
    
    if available_features:
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        df[available_features] = scaler.fit_transform(df[available_features])
        
        print(f"å·²æ ‡å‡†åŒ–ç‰¹å¾: {available_features}")
        print(f"æ ‡å‡†åŒ–åç»Ÿè®¡ä¿¡æ¯:")
        print(df[available_features].describe())
    else:
        print("æœªæ‰¾åˆ°æ•°å€¼ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–")
    
    return df, scaler

# ä½¿ç”¨ç¤ºä¾‹
df, encoders = encode_categorical_features(df)
df, scaler = standardize_numeric_features(df)
```

#### 6.1.3 æ•°æ®è´¨é‡æ£€æŸ¥
```python
def check_data_quality(df):
    """
    æ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•°
    """
    print("å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥...")
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"æ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(f"  æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"  å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # æ•°æ®ç±»å‹æ£€æŸ¥
    print(f"\næ•°æ®ç±»å‹åˆ†å¸ƒ:")
    print(df.dtypes.value_counts())
    
    # ç¼ºå¤±å€¼æ£€æŸ¥
    missing_info = df.isnull().sum()
    if missing_info.sum() > 0:
        print(f"\nç¼ºå¤±å€¼ä¿¡æ¯:")
        print(missing_info[missing_info > 0])
    else:
        print(f"\næ— ç¼ºå¤±å€¼")
    
    # é‡å¤å€¼æ£€æŸ¥
    duplicates = df.duplicated().sum()
    print(f"\né‡å¤å€¼æ•°é‡: {duplicates}")
    
    # å¼‚å¸¸å€¼æ£€æŸ¥ï¼ˆæ•°å€¼ç‰¹å¾ï¼‰
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    print(f"\næ•°å€¼ç‰¹å¾å¼‚å¸¸å€¼æ£€æŸ¥:")
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]
        print(f"  {col}: {len(outliers)} ä¸ªå¼‚å¸¸å€¼")
    
    print("æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆï¼")
    return df

# ä½¿ç”¨ç¤ºä¾‹
df = check_data_quality(df)
```

#### 6.1.4 ç‰¹å¾å·¥ç¨‹
```python
# é€‰æ‹©æ•°å€¼ç‰¹å¾
numeric_features = ['r-time', 'r-temperature', 'c-time', 'c-temperature', 
                   'Co3+', 'urea', 'NH4F', 'SDS', 'load mass-mg/cm2',
                   'Co-material_encoded', 'F-base_encoded', 'code_encoded']

# æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[numeric_features])

# å‡†å¤‡ç›®æ ‡å˜é‡ï¼ˆå½¢è²Œåˆ†ç±»ï¼‰
y = df['morphology_encoded']
```

### 6.2 æ¨¡å‹è®­ç»ƒè¯¦ç»†æ­¥éª¤

#### 6.2.1 ç‰¹å¾é€‰æ‹©ï¼ˆRFEï¼‰
```python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb

# å®šä¹‰åŸºæ¨¡å‹
rf = RandomForestClassifier(n_estimators=100, random_state=42)
xgb_model = xgb.XGBClassifier(random_state=42)
dt = DecisionTreeClassifier(random_state=42)

# ä½¿ç”¨RFEè¿›è¡Œç‰¹å¾é€‰æ‹©
rfe_rf = RFE(estimator=rf, n_features_to_select=8)
rfe_xgb = RFE(estimator=xgb_model, n_features_to_select=8)
rfe_dt = RFE(estimator=dt, n_features_to_select=8)

# è®­ç»ƒRFE
X_rfe_rf = rfe_rf.fit_transform(X_scaled, y)
X_rfe_xgb = rfe_xgb.fit_transform(X_scaled, y)
X_rfe_dt = rfe_dt.fit_transform(X_scaled, y)
```

#### 6.2.2 æ¨¡å‹è®­ç»ƒ
```python
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, classification_report

# åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# è®­ç»ƒå¤šä¸ªæ¨¡å‹
models = {
    'RF-RFE-RF': RandomForestClassifier(n_estimators=100, random_state=42),
    'XGB-RFE-XGB': xgb.XGBClassifier(random_state=42),
    'DT-RFE-DT': DecisionTreeClassifier(random_state=42)
}

# è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
results = {}
for name, model in models.items():
    # ä½¿ç”¨äº¤å‰éªŒè¯
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    
    # è®¡ç®—æŒ‡æ ‡
    results[name] = {
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'test_accuracy': accuracy_score(y_test, y_pred)
    }
```

#### 6.2.3 Stackingæ¨¡å‹æ„å»º
```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier

# å®šä¹‰åŸºæ¨¡å‹
base_models = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgb', xgb.XGBClassifier(random_state=42)),
    ('dt', DecisionTreeClassifier(random_state=42))
]

# æ„å»ºStackingæ¨¡å‹
stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=LogisticRegression(),
    cv=5
)

# è®­ç»ƒStackingæ¨¡å‹
stacking_model.fit(X_train, y_train)
y_pred_stacking = stacking_model.predict(X_test)
```

### 6.3 æ¨¡å‹æµ‹è¯•è¯¦ç»†æ­¥éª¤

#### 6.3.1 æ€§èƒ½è¯„ä¼°
```python
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred_stacking)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix - Stacking Model')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# åˆ†ç±»æŠ¥å‘Š
print("Classification Report:")
print(classification_report(y_test, y_pred_stacking))
```

#### 6.3.2 äº¤å‰éªŒè¯ç»“æœ
```python
# å¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯
cv_results = {}
for name, model in models.items():
    cv_scores = cross_val_score(model, X_scaled, y, cv=5)
    cv_results[name] = {
        'mean': cv_scores.mean(),
        'std': cv_scores.std(),
        'scores': cv_scores
    }

# å¯è§†åŒ–äº¤å‰éªŒè¯ç»“æœ
plt.figure(figsize=(10, 6))
names = list(cv_results.keys())
means = [cv_results[name]['mean'] for name in names]
stds = [cv_results[name]['std'] for name in names]

plt.bar(names, means, yerr=stds, capsize=5)
plt.title('Cross-Validation Results Comparison')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

## ä¸ƒã€ç»“æœåˆ†æ

### 7.1 æµ‹è¯•ç»“æœå±•ç¤º

#### 7.1.1 æ¨¡å‹æ€§èƒ½å¯¹æ¯”
æ ¹æ®é¡¹ç›®æ‘˜è¦ï¼Œå„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹ï¼š

| æ¨¡å‹åç§° | RÂ² | RMSE (F/g) | MAE (F/g) |
|---------|----|------------|-----------|
| XGB-RFE-XGB | 0.95 | 111.83 | 68.25 |
| RF-RFE-RF | 0.92 | 125.67 | 75.43 |
| DT-RFE-DT | 0.88 | 145.23 | 89.12 |

#### 7.1.2 å½¢è²Œåˆ†ç±»ç»“æœ
Stackingèåˆæ¨¡å‹åœ¨å½¢è²Œåˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼š
- **æ•´ä½“å‡†ç¡®ç‡**ï¼šçº¦85-90%
- **1Då½¢è²Œè¯†åˆ«å‡†ç¡®ç‡**ï¼šçº¦88%
- **2Då½¢è²Œè¯†åˆ«å‡†ç¡®ç‡**ï¼šçº¦87%
- **3Då½¢è²Œè¯†åˆ«å‡†ç¡®ç‡**ï¼šçº¦86%

### 7.2 å„å¸¸ç”¨è¯„ä»·æŒ‡æ ‡è®¡ç®—

#### 7.2.1 å›å½’ä»»åŠ¡è¯„ä»·æŒ‡æ ‡
```python
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

def calculate_regression_metrics(y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    
    return {
        'RÂ²': r2,
        'RMSE': rmse,
        'MAE': mae
    }
```

#### 7.2.2 åˆ†ç±»ä»»åŠ¡è¯„ä»·æŒ‡æ ‡
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def calculate_classification_metrics(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    
    return {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1
    }
```

### 7.3 ä¸å…¶ä»–æ¨¡å‹çš„ç»“æœå¯¹æ¯”

#### 7.3.1 ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”
- **ä¼ ç»Ÿå®éªŒæ–¹æ³•**ï¼šéœ€è¦å¤§é‡è¯•é”™å®éªŒï¼Œæˆæœ¬é«˜ï¼Œå‘¨æœŸé•¿
- **æœºå™¨å­¦ä¹ æ–¹æ³•**ï¼šèƒ½å¤Ÿå¿«é€Ÿé¢„æµ‹ï¼Œæˆæœ¬ä½ï¼Œå¯é‡å¤æ€§å¼º

#### 7.3.2 ä¸åŒMLç®—æ³•å¯¹æ¯”
- **XGBoost**ï¼šåœ¨ç‰¹å¾é‡è¦æ€§åˆ†æå’Œé¢„æµ‹ç²¾åº¦æ–¹é¢è¡¨ç°æœ€ä½³
- **éšæœºæ£®æ—**ï¼šç¨³å®šæ€§å¥½ï¼Œä½†é¢„æµ‹ç²¾åº¦ç•¥ä½äºXGBoost
- **å†³ç­–æ ‘**ï¼šå¯è§£é‡Šæ€§å¼ºï¼Œä½†å®¹æ˜“è¿‡æ‹Ÿåˆ

### 7.4 é¡¹ç›®å®ç°æ–¹æ³•å’Œç»“æœåˆ†æ

#### 7.4.1 ä¼˜ç‚¹åˆ†æ
1. **é¢„æµ‹ç²¾åº¦é«˜**ï¼šXGB-RFE-XGBæ¨¡å‹RÂ²è¾¾åˆ°0.95ï¼Œé¢„æµ‹ç²¾åº¦ä¼˜ç§€
2. **ç‰¹å¾é€‰æ‹©æœ‰æ•ˆ**ï¼šRFEæ–¹æ³•æˆåŠŸè¯†åˆ«å‡ºå…³é”®å½±å“å› ç´ 
3. **æ¨¡å‹èåˆæˆåŠŸ**ï¼šStackingæ¨¡å‹åœ¨å½¢è²Œåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚
4. **å¯è§£é‡Šæ€§å¼º**ï¼šSHAPåˆ†ææä¾›äº†æ¸…æ™°çš„æœºç†è§£é‡Š
5. **å®ç”¨ä»·å€¼é«˜**ï¼šä¸ºå®éªŒè®¾è®¡æä¾›äº†å…·ä½“æŒ‡å¯¼

#### 7.4.2 å±€é™æ€§åˆ†æ
1. **æ•°æ®ä¾èµ–æ€§**ï¼šæ¨¡å‹æ€§èƒ½é«˜åº¦ä¾èµ–äºè®­ç»ƒæ•°æ®è´¨é‡å’Œæ•°é‡
2. **ç‰¹å¾å·¥ç¨‹**ï¼šéœ€è¦é¢†åŸŸä¸“å®¶çŸ¥è¯†è¿›è¡Œç‰¹å¾é€‰æ‹©å’Œå·¥ç¨‹
3. **æ¨¡å‹å¤æ‚åº¦**ï¼šé›†æˆæ¨¡å‹çš„å¯è§£é‡Šæ€§ç›¸å¯¹è¾ƒä½
4. **æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨æ–°ææ–™ä½“ç³»ä¸Šçš„æ³›åŒ–èƒ½åŠ›éœ€è¦éªŒè¯

#### 7.4.3 æœªæ¥å±•æœ›
1. **æ•°æ®æ‰©å……**ï¼šæ”¶é›†æ›´å¤šæ ·åŒ–çš„å®éªŒæ•°æ®ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›
2. **ç®—æ³•ä¼˜åŒ–**ï¼šå°è¯•æ·±åº¦å­¦ä¹ ç­‰æ›´å…ˆè¿›çš„ç®—æ³•
3. **åœ¨çº¿å­¦ä¹ **ï¼šæ„å»ºåœ¨çº¿å­¦ä¹ ç³»ç»Ÿï¼ŒæŒç»­ä¼˜åŒ–æ¨¡å‹æ€§èƒ½
4. **å¤šç›®æ ‡ä¼˜åŒ–**ï¼šåŒæ—¶ä¼˜åŒ–å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚æ¯”ç”µå®¹ã€å¾ªç¯ç¨³å®šæ€§ç­‰
5. **å®éªŒéªŒè¯**ï¼šé€šè¿‡å®éªŒéªŒè¯æ¨¡å‹é¢„æµ‹ç»“æœçš„å‡†ç¡®æ€§

### 7.5 é¡¹ç›®å¤ç°å»ºè®®

#### 7.5.1 æ•°æ®å‡†å¤‡
- ç¡®ä¿æ•°æ®æ–‡ä»¶è·¯å¾„æ­£ç¡®
- æ£€æŸ¥æ•°æ®æ ¼å¼å’Œç¼–ç 
- éªŒè¯æ•°æ®å®Œæ•´æ€§

#### 7.5.2 ç¯å¢ƒé…ç½®
- ä¸¥æ ¼æŒ‰ç…§ç¯å¢ƒè¦æ±‚å®‰è£…ä¾èµ–åŒ…
- æ³¨æ„Pythonç‰ˆæœ¬å…¼å®¹æ€§
- å®šæœŸæ›´æ–°condaç¯å¢ƒ

#### 7.5.3 æ¨¡å‹è°ƒä¼˜
- æ ¹æ®å…·ä½“æ•°æ®è°ƒæ•´è¶…å‚æ•°
- ä½¿ç”¨ç½‘æ ¼æœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–
- æ³¨æ„è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆé—®é¢˜

#### 7.5.4 ç»“æœéªŒè¯
- ä½¿ç”¨äº¤å‰éªŒè¯ç¡®ä¿ç»“æœç¨³å®šæ€§
- å¯¹æ¯”ä¸åŒéšæœºç§å­çš„ç»“æœ
- åˆ†ææ¨¡å‹é¢„æµ‹é”™è¯¯çš„æ¡ˆä¾‹